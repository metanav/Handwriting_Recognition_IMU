{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from os import listdir, path\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import datetime\n",
    "\n",
    "seq_length = 36\n",
    "dim = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def padding(data):\n",
    "    \"\"\"Get neighboor padding.\"\"\"\n",
    "    padded_data = []\n",
    "    noise_level = [ 20, 20, 20, 0.2, 0.2, 0.2 ]\n",
    "    \n",
    "    # Before- Neighbour padding\n",
    "    tmp_data = (np.random.rand(seq_length, dim) - 0.5) * noise_level + data[0]\n",
    "    tmp_data[(seq_length -\n",
    "              min(len(data), seq_length)):] = data[:min(len(data), seq_length)]\n",
    "    padded_data.append(tmp_data)\n",
    "    # After- Neighbour padding\n",
    "    tmp_data = (np.random.rand(seq_length, dim) - 0.5) * noise_level + data[-1]\n",
    "    tmp_data[:min(len(data), seq_length)] = data[:min(len(data), seq_length)]\n",
    "    padded_data.append(tmp_data)\n",
    "    return padded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(data, label):\n",
    "    \"\"\"Support function for format.(Helps format train, valid and test.)\"\"\"\n",
    "    # Add 2 padding, initialize data and label\n",
    "    padded_num = 2\n",
    "    length = len(data) * padded_num\n",
    "    features = np.zeros((length, seq_length, dim))\n",
    "    labels = np.zeros(length)\n",
    "    # Get padding for train, valid and test\n",
    "    for idx, (data, label) in enumerate(zip(data, label)):\n",
    "        padded_data = padding(data)\n",
    "        for num in range(padded_num):\n",
    "            features[padded_num * idx + num] = padded_data[num]\n",
    "            labels[padded_num * idx + num] = label\n",
    "    # Turn into tf.data.Dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features, labels.astype(\"int32\")))\n",
    "    return length, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def time_warping(molecule, denominator, data):\n",
    "  \"\"\"Generate (molecule/denominator)x speed data.\"\"\"\n",
    "  tmp_data = [[0\n",
    "               for i in range(len(data[0]))]\n",
    "              for j in range((int(len(data) / molecule) - 1) * denominator)]\n",
    "  for i in range(int(len(data) / molecule) - 1):\n",
    "    for j in range(len(data[i])):\n",
    "      for k in range(denominator):\n",
    "        tmp_data[denominator * i +\n",
    "                 k][j] = (data[molecule * i + k][j] * (denominator - k) +\n",
    "                          data[molecule * i + k + 1][j] * k) / denominator\n",
    "  return tmp_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def augment_data(original_data, original_label):\n",
    "  \"\"\"Perform data augmentation.\"\"\"\n",
    "  new_data = []\n",
    "  new_label = []\n",
    "  for idx, (data, label) in enumerate(zip(original_data, original_label)):  # pylint: disable=unused-variable\n",
    "    # Original data\n",
    "    new_data.append(data)\n",
    "    new_label.append(label)\n",
    "    # Sequence shift\n",
    "    for num in range(5):  # pylint: disable=unused-variable\n",
    "      new_data.append((np.array(data, dtype=np.float32) +\n",
    "                       (random.random() - 0.5) * 200).tolist())\n",
    "      new_label.append(label)\n",
    "    # Random noise\n",
    "    tmp_data = [[0 for i in range(len(data[0]))] for j in range(len(data))]\n",
    "    for num in range(5):\n",
    "      for i in range(len(tmp_data)):\n",
    "        for j in range(len(tmp_data[i])):\n",
    "          tmp_data[i][j] = data[i][j] + 5 * random.random()\n",
    "      new_data.append(tmp_data)\n",
    "      new_label.append(label)\n",
    "    # Time warping\n",
    "    fractions = [(3, 2), (5, 3), (2, 3), (3, 4), (9, 5), (6, 5), (4, 5)]\n",
    "    for molecule, denominator in fractions:\n",
    "      new_data.append(time_warping(molecule, denominator, data))\n",
    "      new_label.append(label)\n",
    "    # Movement amplification\n",
    "    for molecule, denominator in fractions:\n",
    "      new_data.append(\n",
    "          (np.array(data, dtype=np.float32) * molecule / denominator).tolist())\n",
    "      new_label.append(label)\n",
    "  return new_data, new_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data(data_type, files):\n",
    "    data   = []\n",
    "    labels = []\n",
    "    random.shuffle(files)\n",
    "    \n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            label = path.splitext(file)[0][-1]\n",
    "            labels.append(label)\n",
    "            readings = []\n",
    "            for line in f:\n",
    "                reading = line.strip().split(',')\n",
    "                readings.append([float(i) for i in reading[0:6]])\n",
    "\n",
    "            data.append(readings)\n",
    "            \n",
    "    if data_type == 'train':\n",
    "        data, labels = augment_data(data, labels)\n",
    "    \n",
    "    return build_dataset(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 8.98990727e+02  2.51573364e+02  3.10571131e+02  2.70816742e+01\n",
      "   5.85281014e+00 -1.74726738e+01]\n",
      " [ 9.08521538e+02  2.39759341e+02  3.17019650e+02  2.69394048e+01\n",
      "   5.79981650e+00 -1.75068782e+01]\n",
      " [ 8.96105965e+02  2.39900242e+02  3.14267812e+02  2.70668969e+01\n",
      "   5.78631900e+00 -1.74236732e+01]\n",
      " [ 9.03117475e+02  2.36264405e+02  3.21922351e+02  2.69413800e+01\n",
      "   5.85296206e+00 -1.75274578e+01]\n",
      " [ 9.04556803e+02  2.49923846e+02  3.25924854e+02  2.69415929e+01\n",
      "   5.75412488e+00 -1.75472434e+01]\n",
      " [ 9.03320000e+02  2.45610000e+02  3.19340000e+02  2.69900000e+01\n",
      "   5.82000000e+00 -1.74800000e+01]\n",
      " [ 9.61910000e+02  1.83110000e+02  2.19730000e+02  2.51100000e+01\n",
      "   1.35700000e+01 -2.56000000e+01]\n",
      " [ 1.17383000e+03 -1.80180000e+02  8.33980000e+02 -1.62000000e+01\n",
      "  -4.31300000e+01  4.16900000e+01]\n",
      " [ 9.31150000e+02 -7.34380000e+02  2.81740000e+02 -1.50000000e+00\n",
      "  -2.78900000e+01  5.55300000e+01]\n",
      " [ 4.41410000e+02 -7.63180000e+02 -1.20996000e+03  7.78000000e+00\n",
      "  -3.06900000e+01  8.45900000e+01]\n",
      " [ 7.65620000e+02 -2.95410000e+02  7.77340000e+02  6.81500000e+01\n",
      "   1.60800000e+01  5.09000000e+01]\n",
      " [ 7.24120000e+02 -9.49220000e+02  1.42090000e+02  6.55600000e+01\n",
      "   3.05000000e+01  2.55900000e+01]\n",
      " [ 6.12790000e+02  3.82320000e+02 -6.34800000e+01  5.38500000e+01\n",
      "   5.74700000e+01  1.20500000e+01]\n",
      " [ 8.22750000e+02  9.77000000e+00  1.00146000e+03 -8.35000000e+00\n",
      "   3.01800000e+01  1.50300000e+01]\n",
      " [ 1.49023000e+03  6.88960000e+02  6.83590000e+02 -8.34000000e+00\n",
      "   2.37100000e+01 -7.76000000e+00]\n",
      " [ 1.40723000e+03  1.62110000e+02  4.29690000e+02 -7.31000000e+00\n",
      "   2.30500000e+01 -4.71100000e+01]\n",
      " [ 8.32520000e+02  1.05371000e+03 -4.71190000e+02 -4.98900000e+01\n",
      "   1.07400000e+01 -6.19900000e+01]\n",
      " [ 1.60840000e+03 -6.82130000e+02  1.12891000e+03 -1.12810000e+02\n",
      "  -7.68000000e+00 -5.92300000e+01]\n",
      " [ 1.31445000e+03 -5.61040000e+02 -2.18260000e+02 -3.66400000e+01\n",
      "  -4.70800000e+01 -6.31200000e+01]\n",
      " [ 7.52930000e+02 -3.80900000e+01 -6.92380000e+02 -3.03700000e+01\n",
      "  -6.66900000e+01 -6.29000000e+00]\n",
      " [ 1.58203000e+03  6.50390000e+02  1.40674000e+03 -5.01600000e+01\n",
      "  -7.32200000e+01  2.78200000e+01]\n",
      " [ 1.54492000e+03 -9.49710000e+02  1.51807000e+03 -4.74900000e+01\n",
      "  -6.74500000e+01  5.80000000e+01]\n",
      " [ 1.31396000e+03  4.05270000e+02  1.82620000e+02 -3.23000000e+00\n",
      "  -2.61900000e+01  2.72000000e+01]\n",
      " [ 1.76270000e+02  5.29300000e+02 -2.00000000e+03  1.52600000e+01\n",
      "  -4.98200000e+01  7.33900000e+01]\n",
      " [ 3.27150000e+02 -2.80270000e+02 -3.45700000e+02  1.20300000e+01\n",
      "  -3.32900000e+01  4.11600000e+01]\n",
      " [ 1.10986000e+03  1.54443000e+03  8.86720000e+02  3.62400000e+01\n",
      "   1.70000000e+00 -1.68300000e+01]\n",
      " [ 9.91700000e+02  1.10693000e+03  7.28030000e+02  6.02000000e+00\n",
      "   1.00800000e+01 -2.46800000e+01]\n",
      " [ 9.49710000e+02  2.40230000e+02  2.72950000e+02 -3.94900000e+01\n",
      "  -1.96600000e+01  1.54300000e+01]\n",
      " [ 9.74120000e+02  1.03520000e+02  1.47460000e+02 -5.13800000e+01\n",
      "  -3.90400000e+01  6.46600000e+01]\n",
      " [ 9.88770000e+02 -2.24600000e+01  7.22700000e+01 -2.33500000e+01\n",
      "  -3.53400000e+01  4.01000000e+01]\n",
      " [ 9.95610000e+02  1.75800000e+01  1.08890000e+02  1.84700000e+01\n",
      "  -3.46000000e+00 -9.43000000e+00]\n",
      " [ 9.84860000e+02  7.32400000e+01  1.29390000e+02  1.30000000e-01\n",
      "   1.07600000e+01 -5.11000000e+00]\n",
      " [ 9.74120000e+02  5.90800000e+01  1.87010000e+02 -4.55000000e+00\n",
      "   1.73000000e+00  1.21400000e+01]\n",
      " [ 1.05518000e+03  1.95000000e+00  5.01950000e+02 -1.24400000e+01\n",
      "  -1.34000000e+01  2.67400000e+01]\n",
      " [ 1.36572000e+03  2.43650000e+02  4.63870000e+02 -1.66500000e+01\n",
      "   8.73100000e+01  1.67000000e+01]\n",
      " [ 8.16890000e+02  2.07520000e+02  1.85060000e+02 -6.05000000e+00\n",
      "   8.31600000e+01 -5.90000000e+00]], shape=(36, 6), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "files_path = defaultdict(list)\n",
    "dir = './data'\n",
    "for filename in listdir(dir):\n",
    "    if filename.endswith('.csv'):\n",
    "        digit = path.splitext(filename)[0][-1]\n",
    "        files_path[digit].append(path.join(dir, filename))\n",
    "\n",
    "train_files      = []\n",
    "validation_files = []\n",
    "test_files       = []\n",
    "\n",
    "for digit in files_path:\n",
    "    random.shuffle(files_path[digit])\n",
    "    \n",
    "    train_split = math.floor(len(files_path[digit]) * 0.6) # 60%\n",
    "    validation_split = train_split + math.floor(len(files_path[digit]) * 0.2) # 20%\n",
    "\n",
    "    train_files += files_path[digit][:train_split]\n",
    "    validation_files += files_path[digit][train_split:validation_split]\n",
    "    # remaining 20%\n",
    "    test_files += files_path[digit][validation_split:]\n",
    "\n",
    "train_length, train_data = load_data('train', train_files)\n",
    "validation_length, validation_data = load_data('validation', validation_files)\n",
    "test_length, test_data = load_data('test', test_files )\n",
    "\n",
    "#print(train_length)\n",
    "\n",
    "for (ds, lb) in train_data.take(1):\n",
    "    print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Conv2D(8, (4, 3), padding=\"same\", activation=\"relu\", input_shape=(seq_length, dim, 1)),\n",
    "      tf.keras.layers.MaxPool2D((3, 3)),\n",
    "      tf.keras.layers.Dropout(0.1),\n",
    "      tf.keras.layers.Conv2D(16, (4, 1), padding=\"same\", activation=\"relu\"),\n",
    "      tf.keras.layers.MaxPool2D((3, 1), padding=\"same\"),\n",
    "      tf.keras.layers.Dropout(0.1),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "      tf.keras.layers.Dropout(0.1),\n",
    "      tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 1000 steps, validate for 1 steps\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 10.5381 - accuracy: 0.0641 - val_loss: 2.3030 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 2.3062 - accuracy: 0.0309 - val_loss: 2.3028 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 2.3009 - accuracy: 0.0240 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 2.2966 - accuracy: 0.0860 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 2.2911 - accuracy: 0.1081 - val_loss: 2.3028 - val_accuracy: 0.1000\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 2.2745 - accuracy: 0.1150 - val_loss: 2.2306 - val_accuracy: 0.1250\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 2.2531 - accuracy: 0.1257 - val_loss: 2.2469 - val_accuracy: 0.1250\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 2.2345 - accuracy: 0.1309 - val_loss: 2.2440 - val_accuracy: 0.1250\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 2.2049 - accuracy: 0.1322 - val_loss: 2.2617 - val_accuracy: 0.1000\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 2.1706 - accuracy: 0.1187 - val_loss: 2.2755 - val_accuracy: 0.1250\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 2.1039 - accuracy: 0.1488 - val_loss: 2.4050 - val_accuracy: 0.0750\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 2.0529 - accuracy: 0.1526 - val_loss: 2.4544 - val_accuracy: 0.0500\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 2.0162 - accuracy: 0.1658 - val_loss: 2.8418 - val_accuracy: 0.1250\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.9767 - accuracy: 0.1868 - val_loss: 2.8195 - val_accuracy: 0.1500\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.9049 - accuracy: 0.2276 - val_loss: 2.6535 - val_accuracy: 0.1250\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.8069 - accuracy: 0.2723 - val_loss: 2.5762 - val_accuracy: 0.2500\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.7043 - accuracy: 0.3198 - val_loss: 2.5558 - val_accuracy: 0.2250\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.5634 - accuracy: 0.3937 - val_loss: 2.7014 - val_accuracy: 0.2500\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.3838 - accuracy: 0.4874 - val_loss: 2.5300 - val_accuracy: 0.2750\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.2400 - accuracy: 0.5437 - val_loss: 2.4257 - val_accuracy: 0.4250\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.1557 - accuracy: 0.5733 - val_loss: 2.2870 - val_accuracy: 0.4250\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.0770 - accuracy: 0.6109 - val_loss: 2.1495 - val_accuracy: 0.4000\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.0055 - accuracy: 0.6615 - val_loss: 1.9073 - val_accuracy: 0.3500\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.9516 - accuracy: 0.6851 - val_loss: 2.2925 - val_accuracy: 0.3000\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.8825 - accuracy: 0.7073 - val_loss: 2.3094 - val_accuracy: 0.3500\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.8327 - accuracy: 0.7192 - val_loss: 2.0004 - val_accuracy: 0.3500\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.7989 - accuracy: 0.7294 - val_loss: 1.9737 - val_accuracy: 0.4750\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.7604 - accuracy: 0.7435 - val_loss: 1.8902 - val_accuracy: 0.4500\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.7140 - accuracy: 0.7652 - val_loss: 2.0998 - val_accuracy: 0.3750\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6961 - accuracy: 0.7676 - val_loss: 1.9121 - val_accuracy: 0.4750\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6743 - accuracy: 0.7744 - val_loss: 2.2894 - val_accuracy: 0.4000\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6732 - accuracy: 0.7715 - val_loss: 1.7008 - val_accuracy: 0.5500\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6534 - accuracy: 0.7807 - val_loss: 1.9931 - val_accuracy: 0.4500\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6360 - accuracy: 0.7885 - val_loss: 1.8834 - val_accuracy: 0.4500\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6253 - accuracy: 0.7922 - val_loss: 2.0960 - val_accuracy: 0.4250\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6051 - accuracy: 0.7981 - val_loss: 1.9772 - val_accuracy: 0.3750\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5855 - accuracy: 0.8054 - val_loss: 2.2605 - val_accuracy: 0.4750\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5802 - accuracy: 0.8068 - val_loss: 2.3377 - val_accuracy: 0.4000\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5611 - accuracy: 0.8119 - val_loss: 2.5791 - val_accuracy: 0.3750\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5455 - accuracy: 0.8167 - val_loss: 2.8299 - val_accuracy: 0.3750\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5324 - accuracy: 0.8203 - val_loss: 2.6097 - val_accuracy: 0.3500\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5293 - accuracy: 0.8219 - val_loss: 2.3129 - val_accuracy: 0.4000\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5090 - accuracy: 0.8322 - val_loss: 2.5978 - val_accuracy: 0.2750\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5083 - accuracy: 0.8326 - val_loss: 2.3682 - val_accuracy: 0.4500\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5012 - accuracy: 0.8356 - val_loss: 2.6630 - val_accuracy: 0.3500\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4966 - accuracy: 0.8363 - val_loss: 2.1969 - val_accuracy: 0.3750\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4849 - accuracy: 0.8401 - val_loss: 2.5905 - val_accuracy: 0.3750\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4892 - accuracy: 0.8377 - val_loss: 2.0610 - val_accuracy: 0.3000\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4848 - accuracy: 0.8411 - val_loss: 2.3907 - val_accuracy: 0.3750\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4852 - accuracy: 0.8417 - val_loss: 2.1999 - val_accuracy: 0.4500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13861c0d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 50\n",
    "batch_size = 64\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "def reshape_function(data, label):\n",
    "  reshaped_data = tf.reshape(data, [-1, 6, 1])\n",
    "  return reshaped_data, label\n",
    "\n",
    "train_data = train_data.map(reshape_function)\n",
    "validation_data = validation_data.map(reshape_function)\n",
    "\n",
    "train_data = train_data.batch(batch_size).repeat()\n",
    "validation_data = validation_data.batch(batch_size)\n",
    "\n",
    "logdir = \"logs/scalars/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "model.fit(\n",
    "  train_data,\n",
    "  epochs=epochs,\n",
    "  validation_data=validation_data,\n",
    "  steps_per_epoch=1000,\n",
    "  validation_steps=int((validation_length - 1) / batch_size + 1),\n",
    "  callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 3ms/step - loss: 1.6854 - accuracy: 0.5000\n",
      "tf.Tensor(\n",
      "[[2 0 0 0 2 0 0 0 0 0]\n",
      " [0 0 1 0 1 1 1 0 0 0]\n",
      " [0 0 4 0 0 0 0 0 0 0]\n",
      " [0 0 1 3 0 0 0 0 0 0]\n",
      " [0 0 1 0 1 2 0 0 0 0]\n",
      " [0 0 0 0 0 1 2 0 0 1]\n",
      " [0 0 0 0 0 0 4 0 0 0]\n",
      " [0 1 0 1 0 0 0 2 0 0]\n",
      " [0 0 0 0 1 1 0 0 2 0]\n",
      " [0 2 0 0 0 0 1 0 0 1]], shape=(10, 10), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "test_data = test_data.map(reshape_function)\n",
    "test_labels = np.zeros(test_length)\n",
    "\n",
    "idx = 0\n",
    "for data, label in test_data:\n",
    "    test_labels[idx] = label.numpy()\n",
    "    idx += 1\n",
    "    \n",
    "test_data = test_data.batch(batch_size)\n",
    "\n",
    "loss, acc = model.evaluate(test_data)\n",
    "pred = np.argmax(model.predict(test_data), axis=1)\n",
    "confusion = tf.math.confusion_matrix(labels=tf.constant(test_labels), predictions=tf.constant(pred), num_classes=10)\n",
    "print(confusion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
