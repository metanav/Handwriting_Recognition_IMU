{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from os import listdir, path\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "random.seed(42) # Keep the order stable everytime shuffling the files while creating training datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length  = 36 # This will be used to keep the fixed input size for the first CNN layer\n",
    "dim         = 6  # Number of datapoints in a single reading accX,accY,accZ,gyrX,gyrY,gyrZ\n",
    "num_classes = 10 # Number of output classes [0,9] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Padding\n",
    "#### When collecting sequence data, individual samples have different lengths. Since the input data for a convolutional neural network  must be a single tensor, samples need to be padded. The sequence are padded at the beginning and at the end with neighboring values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def padding(data):\n",
    "    padded_data = []\n",
    "    noise_level = [ 20, 20, 20, 0.2, 0.2, 0.2 ]\n",
    "    \n",
    "    tmp_data = (np.random.rand(seq_length, dim) - 0.5) * noise_level + data[0]\n",
    "    tmp_data[(seq_length - min(len(data), seq_length)):] = data[:min(len(data), seq_length)]\n",
    "    padded_data.append(tmp_data)\n",
    "\n",
    "    tmp_data = (np.random.rand(seq_length, dim) - 0.5) * noise_level + data[-1]\n",
    "    tmp_data[:min(len(data), seq_length)] = data[:min(len(data), seq_length)]\n",
    "    \n",
    "    padded_data.append(tmp_data)\n",
    "    return padded_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to TensorFlow dataset, keeps data and labels together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(data, label):\n",
    "    # Add 2 padding, initialize data and label\n",
    "    padded_num = 2\n",
    "    length = len(data) * padded_num\n",
    "    features = np.zeros((length, seq_length, dim))\n",
    "    labels = np.zeros(length)\n",
    "    # Get padding for train, valid and test\n",
    "    for idx, (data, label) in enumerate(zip(data, label)):\n",
    "        padded_data = padding(data)\n",
    "        for num in range(padded_num):\n",
    "            features[padded_num * idx + num] = padded_data[num]\n",
    "            labels[padded_num * idx + num] = label\n",
    "    # Turn into tf.data.Dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features, labels.astype(\"int32\")))\n",
    "    return length, dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Warping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def time_warping(molecule, denominator, data):\n",
    "  tmp_data = [[0 for i in range(len(data[0]))] for j in range((int(len(data) / molecule) - 1) * denominator)]\n",
    "    \n",
    "  for i in range(int(len(data) / molecule) - 1):\n",
    "    for j in range(len(data[i])):\n",
    "      for k in range(denominator):\n",
    "        tmp_data[denominator * i + k][j] = (data[molecule * i + k][j] * (denominator - k) \n",
    "                                            + data[molecule * i + k + 1][j] * k) / denominator\n",
    "  return tmp_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def augment_data(original_data, original_label):\n",
    "  new_data = []\n",
    "  new_label = []\n",
    "  for idx, (data, label) in enumerate(zip(original_data, original_label)):  # pylint: disable=unused-variable\n",
    "    # Original data\n",
    "    new_data.append(data)\n",
    "    new_label.append(label)\n",
    "    # Shift Sequence\n",
    "    for num in range(5):  # pylint: disable=unused-variable\n",
    "      new_data.append((np.array(data, dtype=np.float32) +\n",
    "                       (random.random() - 0.5) * 200).tolist())\n",
    "      new_label.append(label)\n",
    "    # Add Random noise\n",
    "    tmp_data = [[0 for i in range(len(data[0]))] for j in range(len(data))]\n",
    "    for num in range(5):\n",
    "      for i in range(len(tmp_data)):\n",
    "        for j in range(len(tmp_data[i])):\n",
    "          tmp_data[i][j] = data[i][j] + 5 * random.random()\n",
    "      new_data.append(tmp_data)\n",
    "      new_label.append(label)\n",
    "    # Time warping\n",
    "    fractions = [(3, 2), (5, 3), (2, 3), (3, 4), (9, 5), (6, 5), (4, 5)]\n",
    "    for molecule, denominator in fractions:\n",
    "      new_data.append(time_warping(molecule, denominator, data))\n",
    "      new_label.append(label)\n",
    "    # Movement amplification\n",
    "    for molecule, denominator in fractions:\n",
    "      new_data.append(\n",
    "          (np.array(data, dtype=np.float32) * molecule / denominator).tolist())\n",
    "      new_label.append(label)\n",
    "  return new_data, new_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data(data_type, files):\n",
    "    data   = []\n",
    "    labels = []\n",
    "    random.shuffle(files)\n",
    "   \n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            label = path.splitext(file)[0][-1]\n",
    "            labels.append(label)\n",
    "            readings = []\n",
    "            for line in f:\n",
    "                reading = line.strip().split(',')\n",
    "                readings.append([float(i) for i in reading[0:6]])\n",
    "\n",
    "            data.append(readings)\n",
    "            \n",
    "    if data_type == 'train':\n",
    "        data, labels = augment_data(data, labels)\n",
    "    \n",
    "    return build_dataset(data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training, validation, and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files_path = defaultdict(list)\n",
    "dir = './data'\n",
    "for filename in listdir(dir):\n",
    "    if filename.endswith('.csv'):\n",
    "        digit = path.splitext(filename)[0][-1]\n",
    "        files_path[digit].append(path.join(dir, filename))\n",
    "\n",
    "train_files      = []\n",
    "validation_files = []\n",
    "test_files       = []\n",
    "\n",
    "for digit in files_path:\n",
    "    random.shuffle(files_path[digit])\n",
    "    \n",
    "    train_split = int(len(files_path[digit]) * 0.6) # 60%\n",
    "    validation_split = train_split + int(len(files_path[digit]) * 0.2) # 20%\n",
    "\n",
    "    train_files += files_path[digit][:train_split]\n",
    "    validation_files += files_path[digit][train_split:validation_split]\n",
    "    # remaining 20%\n",
    "    test_files += files_path[digit][validation_split:]\n",
    "\n",
    "train_length, train_data = load_data('train', train_files)\n",
    "validation_length, validation_data = load_data('validation', validation_files)\n",
    "test_length, test_data = load_data('test', test_files )\n",
    "\n",
    "print('train_length={} validation_length={} test_length{}'.format(train_length, validation_length, test_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Conv2D(8, (3, 3), padding=\"same\", activation=\"relu\", input_shape=(seq_length, dim, 1)),\n",
    "      tf.keras.layers.Conv2D(8, (3, 3), padding=\"same\", activation=\"relu\"),\n",
    "      tf.keras.layers.MaxPool2D((2, 2)),\n",
    "      tf.keras.layers.Dropout(0.1),\n",
    "      tf.keras.layers.Conv2D(8, (3, 3), padding=\"same\", activation=\"relu\"),\n",
    "      tf.keras.layers.MaxPool2D((2, 2), padding=\"same\"),\n",
    "      tf.keras.layers.Dropout(0.1),\n",
    "      tf.keras.layers.Conv2D(16, (3, 3), padding=\"same\", activation=\"relu\"),\n",
    "      tf.keras.layers.MaxPool2D((2, 2), padding=\"same\"),\n",
    "      tf.keras.layers.Dropout(0.1),\n",
    "      tf.keras.layers.Conv2D(16, (3, 3), padding=\"same\", activation=\"relu\"),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n",
    "  ])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 64\n",
    "steps_per_epoch=1000\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "def reshape_function(data, label):\n",
    "  reshaped_data = tf.reshape(data, [-1, dim, 1])\n",
    "  return reshaped_data, label\n",
    "\n",
    "train_data = train_data.map(reshape_function)\n",
    "validation_data = validation_data.map(reshape_function)\n",
    "\n",
    "train_data = train_data.batch(batch_size).repeat()\n",
    "validation_data = validation_data.batch(batch_size)\n",
    "\n",
    "logdir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "# Uncomment the ines below if you like to see how training proceeds\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir logdir\n",
    "\n",
    "model.fit(\n",
    "  train_data,\n",
    "  epochs=epochs,\n",
    "  validation_data=validation_data,\n",
    "  steps_per_epoch=steps_per_epoch,\n",
    "  validation_steps=int((validation_length - 1) / batch_size + 1),\n",
    "  callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the trained model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_data = test_data.map(reshape_function)\n",
    "test_labels = np.zeros(test_length)\n",
    "\n",
    "# There is no easy function to get the labels back from the tf.data.Dataset :(\n",
    "# Need to iterate over dataset\n",
    "idx = 0\n",
    "for data, label in test_data:\n",
    "    test_labels[idx] = label.numpy()\n",
    "    idx += 1\n",
    "    \n",
    "test_data = test_data.batch(batch_size)\n",
    "\n",
    "loss, acc = model.evaluate(test_data)\n",
    "pred = np.argmax(model.predict(test_data), axis=1)\n",
    "\n",
    "# Create a confusion matrix to see how model predicts\n",
    "confusion = tf.math.confusion_matrix(labels=tf.constant(test_labels), predictions=tf.constant(pred), num_classes=num_classes)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert model to TFLite format \n",
    "### Note: Currently quantized TFLite format does not work with TFLite Micro library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "open(\"model.tflite\", \"wb\").write(tflite_model)\n",
    "\n",
    "# Convert the model to the TensorFlow Lite format with quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "tflite_model = converter.convert()\n",
    "open(\"model_quantized.tflite\", \"wb\").write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
