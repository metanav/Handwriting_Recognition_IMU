{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from os import listdir, path\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import datetime\n",
    "\n",
    "random.seed(42)\n",
    "seq_length = 40\n",
    "dim = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def padding(data):\n",
    "    \"\"\"Get neighboor padding.\"\"\"\n",
    "    padded_data = []\n",
    "    noise_level = [ 20, 20, 20, 0.2, 0.2, 0.2 ]\n",
    "    \n",
    "    # Before- Neighbour padding\n",
    "    tmp_data = (np.random.rand(seq_length, dim) - 0.5) * noise_level + data[0]\n",
    "    tmp_data[(seq_length -\n",
    "              min(len(data), seq_length)):] = data[:min(len(data), seq_length)]\n",
    "    padded_data.append(tmp_data)\n",
    "    # After- Neighbour padding\n",
    "    tmp_data = (np.random.rand(seq_length, dim) - 0.5) * noise_level + data[-1]\n",
    "    tmp_data[:min(len(data), seq_length)] = data[:min(len(data), seq_length)]\n",
    "    padded_data.append(tmp_data)\n",
    "    return padded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(data, label):\n",
    "    \"\"\"Support function for format.(Helps format train, valid and test.)\"\"\"\n",
    "    # Add 2 padding, initialize data and label\n",
    "    padded_num = 2\n",
    "    length = len(data) * padded_num\n",
    "    features = np.zeros((length, seq_length, dim))\n",
    "    labels = np.zeros(length)\n",
    "    # Get padding for train, valid and test\n",
    "    for idx, (data, label) in enumerate(zip(data, label)):\n",
    "        padded_data = padding(data)\n",
    "        for num in range(padded_num):\n",
    "            features[padded_num * idx + num] = padded_data[num]\n",
    "            labels[padded_num * idx + num] = label\n",
    "    # Turn into tf.data.Dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features, labels.astype(\"int32\")))\n",
    "    return length, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def time_warping(molecule, denominator, data):\n",
    "  \"\"\"Generate (molecule/denominator)x speed data.\"\"\"\n",
    "  tmp_data = [[0\n",
    "               for i in range(len(data[0]))]\n",
    "              for j in range((int(len(data) / molecule) - 1) * denominator)]\n",
    "  for i in range(int(len(data) / molecule) - 1):\n",
    "    for j in range(len(data[i])):\n",
    "      for k in range(denominator):\n",
    "        tmp_data[denominator * i +\n",
    "                 k][j] = (data[molecule * i + k][j] * (denominator - k) +\n",
    "                          data[molecule * i + k + 1][j] * k) / denominator\n",
    "  return tmp_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def augment_data(original_data, original_label):\n",
    "  \"\"\"Perform data augmentation.\"\"\"\n",
    "  new_data = []\n",
    "  new_label = []\n",
    "  for idx, (data, label) in enumerate(zip(original_data, original_label)):  # pylint: disable=unused-variable\n",
    "    # Original data\n",
    "    new_data.append(data)\n",
    "    new_label.append(label)\n",
    "    # Sequence shift\n",
    "    for num in range(5):  # pylint: disable=unused-variable\n",
    "      new_data.append((np.array(data, dtype=np.float32) +\n",
    "                       (random.random() - 0.5) * 200).tolist())\n",
    "      new_label.append(label)\n",
    "    # Random noise\n",
    "    tmp_data = [[0 for i in range(len(data[0]))] for j in range(len(data))]\n",
    "    for num in range(5):\n",
    "      for i in range(len(tmp_data)):\n",
    "        for j in range(len(tmp_data[i])):\n",
    "          tmp_data[i][j] = data[i][j] + 5 * random.random()\n",
    "      new_data.append(tmp_data)\n",
    "      new_label.append(label)\n",
    "    # Time warping\n",
    "    fractions = [(3, 2), (5, 3), (2, 3), (3, 4), (9, 5), (6, 5), (4, 5)]\n",
    "    for molecule, denominator in fractions:\n",
    "      new_data.append(time_warping(molecule, denominator, data))\n",
    "      new_label.append(label)\n",
    "    # Movement amplification\n",
    "    for molecule, denominator in fractions:\n",
    "      new_data.append(\n",
    "          (np.array(data, dtype=np.float32) * molecule / denominator).tolist())\n",
    "      new_label.append(label)\n",
    "  return new_data, new_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data(data_type, files):\n",
    "    data   = []\n",
    "    labels = []\n",
    "    random.shuffle(files)\n",
    "    \n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            label = path.splitext(file)[0][-1]\n",
    "            labels.append(label)\n",
    "            readings = []\n",
    "            for line in f:\n",
    "                reading = line.strip().split(',')\n",
    "                readings.append([float(i) for i in reading[0:6]])\n",
    "\n",
    "            data.append(readings)\n",
    "            \n",
    "    #if data_type == 'train':\n",
    "        #data, labels = augment_data(data, labels)\n",
    "    \n",
    "    return build_dataset(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 40 40\n",
      "tf.Tensor(\n",
      "[[ 8.58776867e+02 -9.64611596e+01  5.29959143e+02 -8.01293340e-01\n",
      "  -2.24079661e+01 -3.74029432e+00]\n",
      " [ 8.66848130e+02 -1.06877687e+02  5.30007849e+02 -7.48403183e-01\n",
      "  -2.24246152e+01 -3.81076774e+00]\n",
      " [ 8.54553214e+02 -1.06163804e+02  5.25145795e+02 -7.96543459e-01\n",
      "  -2.24170970e+01 -3.85714578e+00]\n",
      " [ 8.48982102e+02 -9.96251427e+01  5.23474362e+02 -6.37492171e-01\n",
      "  -2.24975155e+01 -3.87392787e+00]\n",
      " [ 8.52574715e+02 -1.04975429e+02  5.35612770e+02 -6.42654750e-01\n",
      "  -2.23914031e+01 -3.89449329e+00]\n",
      " [ 8.58890000e+02 -1.04000000e+02  5.28810000e+02 -7.10000000e-01\n",
      "  -2.24400000e+01 -3.80000000e+00]\n",
      " [ 9.03810000e+02 -1.50390000e+02  3.87210000e+02 -1.70300000e+01\n",
      "  -2.97300000e+01 -1.68000000e+00]\n",
      " [ 8.43750000e+02 -8.49600000e+01  3.59380000e+02  1.59000000e+01\n",
      "  -3.39800000e+01 -4.53000000e+00]\n",
      " [ 9.25780000e+02 -1.54300000e+02  2.64650000e+02  1.70800000e+01\n",
      "  -4.19800000e+01 -1.41400000e+01]\n",
      " [ 1.10449000e+03 -1.69430000e+02  4.70210000e+02  2.99900000e+01\n",
      "  -2.89100000e+01 -1.36600000e+01]\n",
      " [ 9.75100000e+02 -1.63090000e+02  2.43650000e+02  2.20500000e+01\n",
      "  -1.91800000e+01  2.09000000e+00]\n",
      " [ 1.23828000e+03 -3.85700000e+01  3.96970000e+02  1.50100000e+01\n",
      "  -2.64700000e+01  1.75800000e+01]\n",
      " [ 1.46680000e+03 -1.12300000e+02  7.90530000e+02  2.14600000e+01\n",
      "  -1.88200000e+01  4.98000000e+00]\n",
      " [ 7.32420000e+02 -6.87990000e+02  1.37210000e+02  2.16600000e+01\n",
      "   2.90000000e-01  2.08000000e+00]\n",
      " [ 1.13135000e+03 -1.84080000e+02  3.78910000e+02  1.31500000e+01\n",
      "  -3.62700000e+01  2.83700000e+01]\n",
      " [ 1.24854000e+03  1.73830000e+02  1.03857000e+03  8.73000000e+00\n",
      "  -2.94800000e+01  2.55000000e+01]\n",
      " [ 1.14307000e+03  3.76950000e+02  5.34180000e+02 -5.69000000e+00\n",
      "  -4.16300000e+01  2.44400000e+01]\n",
      " [ 8.90140000e+02 -2.61230000e+02  2.37300000e+02 -2.52300000e+01\n",
      "  -3.95200000e+01  1.62700000e+01]\n",
      " [ 7.38280000e+02  3.47660000e+02 -1.04346000e+03  1.43900000e+01\n",
      "  -1.38100000e+01 -6.21000000e+00]\n",
      " [ 1.62305000e+03  8.30080000e+02  1.99994000e+03  9.12000000e+00\n",
      "  -1.70400000e+01 -1.32100000e+01]\n",
      " [ 7.02640000e+02 -5.27300000e+01 -7.13870000e+02 -1.15400000e+01\n",
      "  -3.67900000e+01 -2.05900000e+01]\n",
      " [ 8.73540000e+02  8.54980000e+02  2.85640000e+02 -5.88000000e+00\n",
      "  -3.88000000e+01 -2.52100000e+01]\n",
      " [ 1.06494000e+03  5.36130000e+02  2.00200000e+02 -5.45100000e+01\n",
      "  -2.05600000e+01  2.30800000e+01]\n",
      " [ 8.35940000e+02  3.28610000e+02  5.78610000e+02 -3.33800000e+01\n",
      "   1.12000000e+00 -2.66900000e+01]\n",
      " [ 9.24320000e+02 -6.45020000e+02  4.72170000e+02 -3.26300000e+01\n",
      "   6.00000000e-01 -1.68100000e+01]\n",
      " [ 7.34380000e+02  5.56640000e+02  9.14060000e+02 -1.27300000e+01\n",
      "   6.94000000e+00 -1.63400000e+01]\n",
      " [ 6.35740000e+02 -9.60940000e+02 -2.05100000e+01  7.46000000e+00\n",
      "   4.51900000e+01 -4.66000000e+01]\n",
      " [ 1.64746000e+03 -1.54102000e+03 -2.04590000e+02  4.41000000e+00\n",
      "   3.03100000e+01 -1.63000000e+01]\n",
      " [ 1.89844000e+03  1.04490000e+02  4.17970000e+02 -4.28000000e+00\n",
      "   3.79800000e+01  5.41000000e+00]\n",
      " [ 1.99994000e+03  4.54100000e+01 -9.76560000e+02  3.77600000e+01\n",
      "   4.95000000e+01 -1.73400000e+01]\n",
      " [ 8.21290000e+02 -3.40330000e+02  5.37600000e+02  1.25000000e+01\n",
      "   2.54600000e+01  1.65700000e+01]\n",
      " [ 1.79395000e+03  1.94820000e+02 -6.58200000e+02  4.46700000e+01\n",
      "   3.33300000e+01 -4.50000000e-01]\n",
      " [ 1.25928000e+03  8.23730000e+02  3.88180000e+02  7.96000000e+00\n",
      "   1.27800000e+01  2.42300000e+01]\n",
      " [ 2.08500000e+02 -3.28120000e+02  4.86330000e+02  4.13200000e+01\n",
      "  -1.01100000e+01  2.65200000e+01]\n",
      " [ 1.07178000e+03  4.04790000e+02  5.36620000e+02  7.27800000e+01\n",
      "   2.85000000e+00  1.32100000e+01]\n",
      " [ 9.20410000e+02 -1.35250000e+02 -1.10352000e+03  4.31500000e+01\n",
      "   2.42800000e+01 -2.16400000e+01]\n",
      " [ 9.01370000e+02 -1.40430000e+03 -9.02830000e+02 -7.76000000e+00\n",
      "  -5.70900000e+01  3.65000000e+01]\n",
      " [ 9.93160000e+02 -1.90920000e+02  1.48440000e+02 -1.70200000e+01\n",
      "  -9.19200000e+01  4.98200000e+01]\n",
      " [ 1.58154000e+03  2.93460000e+02  6.67970000e+02 -8.81700000e+01\n",
      "   1.46470000e+02  4.16000000e+01]\n",
      " [ 1.21777000e+03  1.52340000e+02  3.43750000e+02  4.31000000e+00\n",
      "   2.25600000e+02 -3.13400000e+01]], shape=(40, 6), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "files_path = defaultdict(list)\n",
    "dir = './data'\n",
    "for filename in listdir(dir):\n",
    "    if filename.endswith('.csv'):\n",
    "        digit = path.splitext(filename)[0][-1]\n",
    "        files_path[digit].append(path.join(dir, filename))\n",
    "\n",
    "train_files      = []\n",
    "validation_files = []\n",
    "test_files       = []\n",
    "\n",
    "for digit in files_path:\n",
    "    random.shuffle(files_path[digit])\n",
    "    \n",
    "    train_split = math.floor(len(files_path[digit]) * 0.6) # 60%\n",
    "    validation_split = train_split + math.floor(len(files_path[digit]) * 0.2) # 20%\n",
    "\n",
    "    train_files += files_path[digit][:train_split]\n",
    "    validation_files += files_path[digit][train_split:validation_split]\n",
    "    # remaining 20%\n",
    "    test_files += files_path[digit][validation_split:]\n",
    "\n",
    "train_length, train_data = load_data('train', train_files)\n",
    "validation_length, validation_data = load_data('validation', validation_files)\n",
    "test_length, test_data = load_data('test', test_files )\n",
    "\n",
    "print(train_length, validation_length, test_length)\n",
    "\n",
    "for (ds, lb) in test_data.take(1):\n",
    "    print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Conv2D(8, (4, 6), padding=\"same\", activation=\"relu\", input_shape=(seq_length, dim, 1)),\n",
    "      tf.keras.layers.MaxPool2D((3, 3)),\n",
    "      tf.keras.layers.Dropout(0.1),\n",
    "      tf.keras.layers.Conv2D(16, (4, 1), padding=\"same\", activation=\"relu\"),\n",
    "      tf.keras.layers.MaxPool2D((3, 1), padding=\"same\"),\n",
    "      tf.keras.layers.Dropout(0.1),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "      tf.keras.layers.Dropout(0.1),\n",
    "      tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 40, 6, 8)          200       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 2, 8)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 13, 2, 8)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 13, 2, 16)         528       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 2, 16)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5, 2, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                2576      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 3,474\n",
      "Trainable params: 3,474\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 1000 steps, validate for 3 steps\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 8.0440 - accuracy: 0.0985 - val_loss: 2.2997 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 2.3156 - accuracy: 0.0998 - val_loss: 2.2993 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 2.3031 - accuracy: 0.0999 - val_loss: 2.2992 - val_accuracy: 0.1000\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2991 - val_accuracy: 0.1000\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.3008 - accuracy: 0.1001 - val_loss: 2.2991 - val_accuracy: 0.1000\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.3046 - accuracy: 0.0999 - val_loss: 2.2991 - val_accuracy: 0.1000\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.2981 - accuracy: 0.1000 - val_loss: 2.2991 - val_accuracy: 0.1000\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.2978 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.3021 - accuracy: 0.1000 - val_loss: 2.2991 - val_accuracy: 0.1000\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 3242s 3s/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.2975 - accuracy: 0.1000 - val_loss: 2.2990 - val_accuracy: 0.1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13b33a050>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "def reshape_function(data, label):\n",
    "  reshaped_data = tf.reshape(data, [-1, 6, 1])\n",
    "  return reshaped_data, label\n",
    "\n",
    "train_data = train_data.map(reshape_function)\n",
    "validation_data = validation_data.map(reshape_function)\n",
    "\n",
    "train_data = train_data.batch(batch_size).repeat()\n",
    "validation_data = validation_data.batch(batch_size)\n",
    "\n",
    "logdir = \"logs/scalars/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "model.fit(\n",
    "  train_data,\n",
    "  epochs=epochs,\n",
    "  validation_data=validation_data,\n",
    "  steps_per_epoch=1000,\n",
    "  validation_steps=int((validation_length - 1) / batch_size + 1),\n",
    "  callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step - loss: 2.3175 - accuracy: 0.1000\n",
      "tf.Tensor(\n",
      "[[0 4 0 0 0 0 0 0 0 0]\n",
      " [0 4 0 0 0 0 0 0 0 0]\n",
      " [0 4 0 0 0 0 0 0 0 0]\n",
      " [0 4 0 0 0 0 0 0 0 0]\n",
      " [0 4 0 0 0 0 0 0 0 0]\n",
      " [0 4 0 0 0 0 0 0 0 0]\n",
      " [0 4 0 0 0 0 0 0 0 0]\n",
      " [0 4 0 0 0 0 0 0 0 0]\n",
      " [0 4 0 0 0 0 0 0 0 0]\n",
      " [0 4 0 0 0 0 0 0 0 0]], shape=(10, 10), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "test_data = test_data.map(reshape_function)\n",
    "test_labels = np.zeros(test_length)\n",
    "\n",
    "idx = 0\n",
    "for data, label in test_data:\n",
    "    test_labels[idx] = label.numpy()\n",
    "    idx += 1\n",
    "    \n",
    "test_data = test_data.batch(batch_size)\n",
    "\n",
    "loss, acc = model.evaluate(test_data)\n",
    "pred = np.argmax(model.predict(test_data), axis=1)\n",
    "confusion = tf.math.confusion_matrix(labels=tf.constant(test_labels), predictions=tf.constant(pred), num_classes=10)\n",
    "print(confusion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
